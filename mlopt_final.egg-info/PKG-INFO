Metadata-Version: 2.4
Name: mlopt-final
Version: 0.1.0
Summary: Machine Learning Optimization for Google Ads Bid Management
Author-email: Author Name <author@example.com>
License: MIT
Project-URL: Homepage, https://github.com/seehanahtang/mlopt-final
Project-URL: Repository, https://github.com/seehanahtang/mlopt-final.git
Project-URL: Issues, https://github.com/seehanahtang/mlopt-final/issues
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: pandas>=1.5.3
Requires-Dist: numpy>=1.24.3
Requires-Dist: scikit-learn>=1.2.2
Requires-Dist: scipy>=1.10.1
Requires-Dist: openpyxl>=3.0.10
Requires-Dist: holidays>=0.83
Requires-Dist: torch>=2.0.0
Provides-Extra: bert
Requires-Dist: sentence-transformers>=5.1.2; extra == "bert"
Requires-Dist: transformers>=4.35.2; extra == "bert"
Provides-Extra: ml
Requires-Dist: iai>=2.11.2; extra == "ml"
Provides-Extra: optimization
Requires-Dist: gurobipy>=10.0.1; extra == "optimization"
Provides-Extra: dev
Requires-Dist: pytest>=7.3.1; extra == "dev"
Requires-Dist: black>=23.3.0; extra == "dev"
Requires-Dist: pylint>=2.12.2; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"

# ML Optimization for Ad Bidding - Project Structure

## Overview

This project implements a machine learning pipeline for optimizing keyword bids in Google Ads. It includes data preprocessing, model training, and bid optimization using linear programming.

## Directory Structure

```
project/
├── raw_data/                       # Raw source data
│   ├── Search keyword report (by Day 2024).xlsx
│   ├── Search keyword report (By day 2025).csv
│   ├── combined_kw_ads_data2.csv  # Google Keyword Planner data
│   ├── unique_keyword_embeddings.csv
│   ├── unique_keywords.csv
│   └── ad_opt_data_w_clicks.parquet
│
├── clean_data/                     # Processed data (generated by pipeline)
│   ├── ad_opt_data_tfidf.csv      # Full dataset with TF-IDF embeddings
│   ├── ad_opt_data_bert.csv       # Full dataset with BERT embeddings
│   ├── train_tfidf.csv            # Training set (TF-IDF)
│   ├── test_tfidf.csv             # Test set (TF-IDF)
│   ├── train_bert.csv             # Training set (BERT)
│   └── test_bert.csv              # Test set (BERT)
│
├── models/                         # Trained models (generated)
│   ├── lr_conversion.json         # Linear regression (conversion)
│   ├── lr_clicks.json             # Linear regression (clicks)
│   ├── oct_conversion.json        # Optimal trees (conversion)
│   ├── oct_clicks.json            # Optimal trees (clicks)
│   ├── rf_conversion.json         # Random forest (conversion)
│   ├── rf_clicks.json             # Random forest (clicks)
│   ├── xgb_conversion.json        # XGBoost (conversion)
│   └── xgb_clicks.json            # XGBoost (clicks)
│
├── notebooks/                      # Jupyter notebooks (reference/development)
│   ├── tidy_get_data.ipynb        # Data exploration & preprocessing
│   ├── tidy_get_data_tfidf.ipynb  # TF-IDF embedding generation
│   ├── prediction_modeling.ipynb  # Model training (Julia)
│   └── bid_optimization.ipynb     # Bid optimization (Julia)
│
├── scripts/                        # Python production scripts
│   ├── tidy_get_data.py           # Data preparation pipeline (main entry)
│   ├── prediction_modeling.py     # Model training (requires IAI library)
│   ├── bid_optimization.py        # Bid optimization (requires Gurobi)
│   └── README.md                  # Scripts documentation
│
├── utils/                          # Reusable utility modules
│   ├── data_cleaning.py           # Currency & percentage parsing
│   ├── date_features.py           # Temporal feature extraction
│   ├── embeddings.py              # TF-IDF & BERT embeddings
│   ├── data_pipeline.py           # High-level pipeline functions
│   ├── __init__.py                # Central export point
│   └── README.md                  # Utils documentation
│
├── helpers.py                      # Backward-compatible re-exports (deprecated)
├── pyproject.toml                  # Project configuration & dependencies
├── README.md                       # This file
└── .git/                           # Git repository
```

## Quick Start

### 1. Setup

```bash
# Clone repository
git clone https://github.com/seehanahtang/mlopt-final.git
cd mlopt-final

# Install core dependencies using pyproject.toml
pip install -e .

# Optional: For BERT embeddings
pip install -e ".[bert]"

# Optional: For model training with IAI
pip install -e ".[ml]"

# Optional: For bid optimization with Gurobi
pip install -e ".[optimization]"

# Or install everything
pip install -e ".[bert,ml,optimization]"
```

**Note on Commercial Licenses:**
- **IAI (InterpretableAI):** Requires commercial license from https://www.interpretable.ai/
- **Gurobi:** Requires commercial license from https://www.gurobi.com/

The script will automatically detect if running on the Engaging cluster (SLURM) and configure IAI appropriately.

### 2. Data Preparation

```bash
# Generate TF-IDF embeddings and prepare datasets
python scripts/tidy_get_data.py --embedding-method tfidf

# Or use BERT embeddings (slower, but more semantic)
python scripts/tidy_get_data.py --embedding-method bert
```

This script:
- Loads and combines 2024 and 2025 keyword data from `raw_data/`
- Cleans currency, percentages, and text
- Extracts temporal features (holidays, weekends, etc.)
- Merges with Google Keyword Planner data
- Generates keyword embeddings (TF-IDF or BERT)
- Removes rows with missing values
- Creates train/test splits (75/25)
- Saves processed datasets to `clean_data/` directory

**Output files:**
- `clean_data/ad_opt_data_tfidf.csv` - Full dataset
- `clean_data/train_tfidf.csv` - Training set (~53k rows)
- `clean_data/test_tfidf.csv` - Test set (~18k rows)

### 3. Train Prediction Models

```bash
# Train models to predict conversion value
python scripts/prediction_modeling.py --target conversion

# Or predict clicks
python scripts/prediction_modeling.py --target clicks

# Train only specific models
python scripts/prediction_modeling.py --target conversion --models lr oct

# Use BERT embeddings
python scripts/prediction_modeling.py --target conversion --embedding-method bert
```

Trains and compares:
- Linear Regression (LR) with feature selection
- Optimal Cart Trees (OCT)
- Random Forests (RF)
- XGBoost (XGB)

**Requirements:** IAI library (`pip install -e ".[ml]"`)

**Features:**
- Automatically uses all columns except targets and metadata
- Converts categorical columns to proper dtype for IAI
- 5-fold cross-validation with hyperparameter grid search
- Saves best model from each training run

**Output:** Models saved to `models/` directory (one per model type and target)

### 4. Optimize Bids

```bash
python scripts/bid_optimization.py \
  --budget 68096.51 \
  --max-bid 100 \
  --max-active 14229 \
  --output optimized_bids.csv
```

Uses Gurobi linear programming to maximize profit given:
- Budget constraint
- Maximum individual bid
- Maximum number of active keywords

**Requirements:** Gurobi solver (commercial license required)

**Output:** `optimized_bids.csv` with recommended bids

## Modules

### `utils/data_cleaning.py`
Utilities for parsing and normalizing data:
- `clean_currency()` - Parse currency strings
- `convert_percent_to_float()` - Parse percentage values

### `utils/date_features.py`
Temporal feature extraction:
- `_is_holiday()` - Detect public holidays
- `calculate_days_to_next()` - Days until next event
- `_region_to_country_code()` - Map regions to country codes

### `utils/embeddings.py`
Keyword embedding generation:
- `get_tfidf_embeddings()` - TF-IDF with TruncatedSVD (fast, interpretable)
- `get_bert_embeddings_pipeline()` - BERT with TruncatedSVD (semantic, accurate)
- `get_bert_embedding()` - Raw BERT embeddings (low-level)

### `utils/data_pipeline.py`
High-level pipeline functions (called by `scripts/tidy_get_data.py`):
- `load_and_combine_keyword_data()`
- `format_keyword_data()`
- `extract_date_features()`
- `merge_with_ads_data()`
- `add_embeddings()`
- `prepare_train_test_split()`
- `save_outputs()`

## Features

### Input Data
- Google Ads performance data (2024-2025)
- Google Keyword Planner data (search volume, competition, bid ranges)

### Engineered Features
**Temporal:**
- Day of week
- Weekend indicator
- Month
- Public holiday indicator
- Days to next course start

**Text/Semantic:**
- TF-IDF embeddings (50 dimensions)
- Or BERT embeddings (50 dimensions after SVD)

**Bid Features:**
- Average CPC
- Average bid (low + high range / 2)

**Categorical:**
- Match type (broad, exact, phrase)
- Region (USA, Region A, B, C)

### Target Variables
- **Conversion Value:** Revenue generated by conversion
- **Clicks:** Number of ad clicks

## Model Performance

Example results (MSE on test set):
```
Conversion Value Prediction:
  Linear Regression:  ~2500
  Optimal Trees:      ~2200
  Random Forest:      ~1800
  XGBoost:            ~1600

Click Prediction:
  Linear Regression:  ~15
  Optimal Trees:      ~12
  Random Forest:      ~8
  XGBoost:            ~6
```

## Optimization

The bid optimization formulates the problem as:

```
maximize:   sum(predicted_conversion - predicted_clicks * bid)
subject to:
  sum(bid) <= budget_limit
  bid[i] <= max_bid * active[i]
  sum(active) <= max_active_keywords
  bid >= 0, active ∈ {0, 1}
```

Where:
- `bid[i]` = CPC bid for keyword i
- `active[i]` = binary variable indicating if keyword is active
- Predictions come from trained models

## Dependencies

Dependencies are managed via `pyproject.toml`. Install using pip with extras:

```bash
# Core dependencies only (data processing, embeddings)
pip install -e .

# Add optional packages as needed
pip install -e ".[bert]"           # BERT embeddings
pip install -e ".[ml]"             # IAI model training
pip install -e ".[optimization]"   # Gurobi optimization
pip install -e ".[dev]"            # Development tools

# All optional packages
pip install -e ".[bert,ml,optimization,dev]"
```

**Core Packages:**
```
pandas>=1.5.3          # Data manipulation
numpy>=1.24.3          # Numerical computing
scikit-learn>=1.2.2    # ML utilities (TF-IDF, train/test split)
scipy>=1.10.1          # Scientific computing
openpyxl>=3.0.10       # Excel file reading
holidays>=0.83         # Holiday calendar detection
torch>=2.0.0           # Deep learning backend
```

**Optional Packages:**
```
# BERT embeddings
sentence-transformers>=5.1.2       # Pre-trained BERT models
transformers>=4.35.2               # HuggingFace transformers (Python 3.9 compatible)

# Model training
iai>=2.11.2                        # InterpretableAI (requires commercial license)

# Bid optimization
gurobipy>=10.0.1                   # Gurobi solver (requires commercial license)

# Development
pytest>=7.3.1                      # Testing
black>=23.3.0                      # Code formatting
pylint>=2.12.2                     # Linting
mypy>=1.0.0                        # Type checking
```

**Note on Commercial Licenses:**
- **IAI:** https://www.interpretable.ai/
- **Gurobi:** https://www.gurobi.com/

## Cluster Support

The `prediction_modeling.py` script automatically detects and configures for the **Engaging cluster** (MIT's SLURM-based cluster):

```bash
# On local machine
python scripts/prediction_modeling.py --target conversion
# Uses: C:\Users\jsitu\IAI\sys.dll (Windows)

# On Engaging cluster (automatically detected via SLURM_NODEID)
python scripts/prediction_modeling.py --target conversion
# Uses: ~/iai/sys.so and Julia from /orcd/software/community/001/pkg/julia/1.10.4/bin/julia
```

No configuration changes needed - the script handles both environments automatically!

## Performance Notes

- **Data preparation:** ~30-60 seconds (TF-IDF), ~2-3 minutes (BERT with GPU)
- **Model training:** ~1-2 minutes per model (5-fold CV)
- **Bid optimization:** ~30 seconds to 2 minutes (depending on Gurobi settings)

## Troubleshooting

### Issue: `ModuleNotFoundError: No module named 'utils'`
**Solution:** Ensure you're running scripts from the project root directory:
```bash
cd /path/to/mlopt-final
python scripts/tidy_get_data.py
```

### Issue: `ImportError: No module named 'iai'`
**Solution:** Install IAI:
```bash
pip install iai
# Note: Requires a valid InterpretableAI license
```

### Issue: `ImportError: No module named 'gurobipy'`
**Solution:** Install Gurobi:
```bash
pip install gurobipy
# Note: Requires a valid Gurobi license
```

### Issue: IAI/Gurobi license errors
**Solution:** Obtain and configure licenses:
- IAI: https://www.interpretable.ai/
- Gurobi: https://www.gurobi.com/

## Workflow

Typical workflow:

```bash
# 1. Prepare data
python scripts/tidy_get_data.py --embedding-method tfidf

# 2. Train models (requires IAI)
python scripts/prediction_modeling.py --target conversion
python scripts/prediction_modeling.py --target clicks

# 3. Optimize bids (requires Gurobi)
python scripts/bid_optimization.py --budget 68096.51

# 4. Review results
head -20 optimized_bids.csv
```

## Further Development

### Potential Improvements
1. **Hyperparameter tuning** - Expand grid search ranges
2. **Ensemble methods** - Combine multiple models
3. **Stochastic optimization** - Account for uncertainty in predictions
4. **Multi-period optimization** - Dynamic bidding over time
5. **A/B testing framework** - Validate recommendations

### Adding New Features
1. Add feature extraction functions to `utils/date_features.py`
2. Update `utils/data_pipeline.py` to compute new features
3. Retrain models with new feature matrix

## References

- Google Ads API: https://developers.google.com/google-ads/api
- InterpretableAI (IAI): https://www.interpretable.ai/
- Gurobi Optimizer: https://www.gurobi.com/
- Scikit-learn: https://scikit-learn.org/
- Sentence-Transformers: https://www.sbert.net/

## License

[Your license here]

## Contact

[Your contact information]
